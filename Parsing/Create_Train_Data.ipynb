{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString \n",
    "import os\n",
    "from os import listdir\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All citation styles considered in the experiment\n",
    "#american-chemical-society\n",
    "#american-medical-association\n",
    "# apa\n",
    "# chicago-annotated-bibliography\n",
    "# ieee\n",
    "# modern-language-association\n",
    "# national-library-of-medicine-grant-proposals\n",
    "# turabian-fullnote-bibliography\n",
    "# vancouver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to check if a file exists\n",
    "def deleteFile( fname ):\n",
    "    if os.path.exists(fname):\n",
    "        os.remove(fname)\n",
    "    else:\n",
    "        print(\"Can not delete the file as it doesn't exists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60:20:20 split for train:test:validation\n",
    "#Path to the input files , after running run_commands notebook\n",
    "def splitData(inputPath,documentType):\n",
    "    filePath = inputPath+documentType\n",
    "\n",
    "    #file name from the run_commands notebook output\n",
    "    file = \"output.no_invalid_rows.txt\"\n",
    "    folders=[]\n",
    "    \n",
    "    #Finding all styles for a particular document type\n",
    "    for r, d, f in os.walk(filePath):\n",
    "        for folder in d:\n",
    "            folders.append(os.path.join(r, folder))\n",
    "    for paths in folders:\n",
    "        fname = paths+\"/\"+file\n",
    "        \n",
    "        num_lines = 0\n",
    "        with open(fname, 'r') as f:\n",
    "            for line in f:\n",
    "                num_lines += 1\n",
    "                \n",
    "        #Calculate the number of lines for each split\n",
    "        #60:40:40 for Train:Val:Test\n",
    "        \n",
    "        train_n = int(0.6*num_lines)\n",
    "        test_n = int(0.2*num_lines)\n",
    "        #print(train_n)\n",
    "        #print(test_n)\n",
    "        \n",
    "        #Reading the input files linewise and append to a list\n",
    "        text = open(fname, 'r')\n",
    "        data = text.readlines()\n",
    "        dict = []\n",
    "        for line in data:\n",
    "            l = line.strip().split('\\n')\n",
    "            dict.append(l)\n",
    "        \n",
    "        #Validation index\n",
    "        val_n = train_n+test_n\n",
    "        print(val_n)\n",
    "        \n",
    "        #Index splicing the data for train test and validation\n",
    "        train_data= dict[0:train_n]\n",
    "        val_data=dict[train_n:val_n]\n",
    "        test_data = dict[val_n:]\n",
    "        print(\"Path is \" + paths)\n",
    "        \n",
    "        #delete if file exists\n",
    "        deleteFile(paths+\"/\"+'train.txt')\n",
    "        deleteFile(paths+\"/\"+'val.txt')    \n",
    "        deleteFile(paths+\"/\"+'test.txt')\n",
    "        \n",
    "        #Write Train data into Disk\n",
    "        file_train = open(paths+\"/\"+'train.txt','w')\n",
    "        for element in train_data:\n",
    "            file_train.write('\\n'.join(element))\n",
    "            file_train.write('\\n')\n",
    "        file_train.close()\n",
    "        \n",
    "        #val_data\n",
    "        file_val = open(paths+\"/\"+'val.txt','w')\n",
    "        for element in val_data:\n",
    "            file_val.write('\\n'.join(element))\n",
    "            file_val.write('\\n')\n",
    "        file_val.close()\n",
    "        \n",
    "        #Test Data \n",
    "        file_test = open(paths+\"/\"+'test.txt','w')\n",
    "        for element in test_data:\n",
    "            file_test.write('\\n'.join(element))\n",
    "            file_test.write('\\n')\n",
    "        file_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3590\n",
      "Path is /Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/journals/modern-language-association\n",
      "3590\n",
      "Path is /Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/journals/apa\n",
      "3590\n",
      "Path is /Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/abc/modern-language-association\n",
      "Can not delete the file as it doesn't exists\n",
      "Can not delete the file as it doesn't exists\n",
      "Can not delete the file as it doesn't exists\n"
     ]
    }
   ],
   "source": [
    "#Path to the input files for all style data\n",
    "inputPath= \"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/\"\n",
    "\n",
    "#Use splitData for all the document types\n",
    "splitData(inputPath,\"journals\")\n",
    "splitData(inputPath,\"proceedings\")\n",
    "splitData(inputPath,\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all train,val,test files for all citation styles into one file for each document type\n",
    "\n",
    "\n",
    "def combine_files(path , documentType):\n",
    "    \n",
    "    filePaths = path+\"/\"+documentType\n",
    "    \n",
    "    train_file_name= \"train.txt\"\n",
    "    test_file_name=\"test.txt\"\n",
    "    val_file_name= \"val.txt\"\n",
    "    folders=[]\n",
    "\n",
    "    for r, d, f in os.walk(filePaths):\n",
    "\n",
    "        for folder in d:\n",
    "            folders.append(os.path.join(r, folder))\n",
    "    for paths in folders:\n",
    "        file= open(paths+\"/\"+train_file_name)\n",
    "        with open(path+\"/\"+documentType+\"_all_train.txt\", \"a\") as f1:\n",
    "            for line in file:\n",
    "                #print(line)\n",
    "                f1.write(line)\n",
    "    for paths in folders:\n",
    "        file= open(paths+\"/\"+ test_file_name)\n",
    "        with open(path+\"/\"+documentType+\"_all_test.txt\", \"a\") as f1:\n",
    "            for line in file:\n",
    "                #print(line)\n",
    "                f1.write(line)\n",
    "    for paths in folders:\n",
    "        file= open(paths+\"/\"+val_file_name)\n",
    "        with open(path+\"/\"+documentType+\"_all_val.txt\", \"a\") as f1:\n",
    "            for line in file:\n",
    "                #print(line)\n",
    "                f1.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Path to all files\n",
    "\n",
    "path = \"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/\"\n",
    "\n",
    "#Add for different documentTypes\n",
    "combine_files(path,'journals')\n",
    "combine_files(path,'proceedings')\n",
    "combine_files(path,'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, glob\n",
    "import shutil\n",
    "\n",
    "#Combine all train test validation from all document type into one single file\n",
    "inputPath= \"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/\"\n",
    "trainFilename =\"all_train.txt\"\n",
    "valFilename=\"all_val.txt\"\n",
    "testFilename=\"all_test.txt\"\n",
    "filenames = glob.glob(inputPath+'*.txt')\n",
    "\n",
    "#TrainFile into one songle train file\n",
    "with open(inputPath+trainFilename, 'wb') as outfile:\n",
    "    for filename in glob.glob(inputPath+'*_train.txt'):\n",
    "     \n",
    "        if filename == inputPath+trainFilename:\n",
    "             continue\n",
    "        with open(filename, 'rb') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "\n",
    "with open(inputPath+testFilename, 'wb') as outfile:\n",
    "    for filename in glob.glob(inputPath+'*_test.txt'):\n",
    "        if filename == inputPath+testFilename:\n",
    "            continue\n",
    "        with open(filename, 'rb') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "            \n",
    "with open(inputPath+valFilename, 'wb') as outfile:\n",
    "    for filename in glob.glob(inputPath+'*_val.txt'):\n",
    "        if filename == inputPath+valFilename:\n",
    "             continue\n",
    "        with open(filename, 'rb') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the train/test/val for each type \n",
    "import random\n",
    "def shuffle_data(path,inputFilename,outputFilename):\n",
    "    lines = open(path+inputFilename).readlines()\n",
    "    random.shuffle(lines)\n",
    "    open(path+outputFilename, 'w').writelines(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/\"\n",
    "trainFile=\"all_train.txt\"\n",
    "trainshuffle=\"train.txt\"\n",
    "valFile=\"all_val.txt\"\n",
    "valShuffle=\"val.txt\"\n",
    "testFile=\"all_test.txt\"\n",
    "testShuffle=\"test.txt\"\n",
    "\n",
    "# Shuffle train, test and valiation data\n",
    "shuffle_data(path,trainFile,trainshuffle)\n",
    "shuffle_data(path,valFile,valShuffle)\n",
    "shuffle_data(path,testFile,testShuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To divide the entire test/train/val into number of folds\n",
    "def create_folds(path,fileType):\n",
    "    fname = path+fileType+\".txt\"\n",
    "    num_lines = 0\n",
    "\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            num_lines += 1\n",
    "    print(num_lines)\n",
    "    window = int(0.1*num_lines)\n",
    "    print(window)\n",
    "    #print(each_fold)\n",
    "\n",
    "\n",
    "    text = open(fname, 'r')\n",
    "    data = text.readlines()\n",
    "    dict = []\n",
    "    for line in data:\n",
    "        l = line.strip().split('\\n')\n",
    "        dict.append(l)\n",
    "    path_to_folds = '/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/folds/'\n",
    "    start_range = 0\n",
    "    temp_range = start_range+window\n",
    "    end_range=len(dict)\n",
    "    for i in range(11):\n",
    "        #fold_i = dict[start_range:end_range]\n",
    "        file = open(path_to_folds+fileType+str(i)+'.txt','w')\n",
    "        \n",
    "       \n",
    "        if not (temp_range > end_range):\n",
    "            #print(file)\n",
    "            print(str(start_range)+\":\"+ str(temp_range))\n",
    "            fold = dict[start_range:temp_range]\n",
    "            for element in fold:\n",
    "                print(element)\n",
    "                file.write('\\n'.join(element))\n",
    "                file.write('\\n')  \n",
    "            start_range = start_range + window\n",
    "            temp_range = start_range + window\n",
    "        else:\n",
    "            print(\"here\")\n",
    "            #print(str(temp_range-window) +\":\"+str(end_range))\n",
    "            last_fold = dict[temp_range-window:end_range]\n",
    "            print(str(temp_range-window)+\":\"+str(end_range))\n",
    "            #print(\"last\"+ str(file))\n",
    "\n",
    "            for last_elem in last_fold:\n",
    "                file.write('\\n'.join(last_elem))\n",
    "                file.write('\\n')     \n",
    "        file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bipashabanerjee/Documents/CS/sem3/publish\r\n"
     ]
    }
   ],
   "source": [
    "path=\"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/\"\n",
    "\n",
    "#Note folders to be created manually\n",
    "!cd styleData/\n",
    "!pwd\n",
    "# create_folds(path,'test')\n",
    "# create_folds(path,'train')\n",
    "# create_folds(path,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate all train and val in folds which is done manually.\n",
    "#the file structure should look like folds/training/{0...9}/{train/val}_{0-9}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_function(inputFile, outputFile):\n",
    "\n",
    "    #write to file the output after stripping the punctuations\n",
    "    #inputFile = \"/Users/bipashabanerjee/Documents/CS/sem3/project/styleData_old/report/modern-language-association/check.txt\"\n",
    "    punctuation = string.punctuation \n",
    "    #print(punctuation)\n",
    "    text = open(inputFile, 'r')\n",
    "    data = text.readlines()\n",
    "    dict = []\n",
    "    for line in data:\n",
    "        l = line.strip().split('\\n')\n",
    "        dict.append(l)\n",
    "    #print(dict)\n",
    "    str1 = str(dict[0])\n",
    "\n",
    "    soup = BeautifulSoup(str1)\n",
    "\n",
    "\n",
    "    tag_list= []\n",
    "    for tag in soup.find_all():\n",
    "        tag_list.append(tag.name)\n",
    "\n",
    "    tag_list.remove('html')\n",
    "    tag_list.remove('body')\n",
    "    tag_list.remove('p')\n",
    "    #To find the tags associated with the particular citation style\n",
    "\n",
    "\n",
    "    for m in dict:\n",
    "        for text in m:\n",
    "\n",
    "            soup1 = BeautifulSoup(text)\n",
    "            body = soup1.find('body')\n",
    "            newtag_list= []\n",
    "            for newtag in body.find_all():\n",
    "                newtag_list.append(newtag.name)\n",
    "    #         for tag in newtag_list:\n",
    "    #             if(newtag_list=='p'):\n",
    "    #                 newtag_list.remove('p')\n",
    "\n",
    "            for text_b in body:\n",
    "                file = open(outputFile,\"a+\")\n",
    "                if isinstance(text_b, NavigableString) == True:\n",
    "                    token = text_b.split()\n",
    "                    for t in token:\n",
    "                        file.write(t + \"\\t\"+ \"extra\"+'\\n')\n",
    "\n",
    "                elif text_b.name =='p':\n",
    "\n",
    "\n",
    "                    newtag_list.remove('p')\n",
    "\n",
    "                    for tag in newtag_list:\n",
    "                        text = soup.find_all(tag)\n",
    "                        for tag_text in text:\n",
    "\n",
    "                            split_text=re.split('\\s+', tag_text.text )\n",
    "                            for t in split_text:\n",
    "                                file.write(t.replace(',','') + \"\\t\"+ tag)\n",
    "    #                         for tag in newtag_list:\n",
    "\n",
    "    #                             text = soup.find_all(tag)\n",
    "    #                             for tag_text in text:\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    split_text=re.split('\\s+', text_b.text )   \n",
    "\n",
    "                    for t in split_text:\n",
    "\n",
    "                        if text_b.name == \"author\":\n",
    "\n",
    "                            if(t in punctuation):\n",
    "                                file.write(t+\"\\t\"+'punctuation'+'\\n')\n",
    "                            else:\n",
    "\n",
    "                                file.write(t.replace(',','')+\"\\t\"+text_b.name+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            if(t!='' and t ):\n",
    "                                file.write(t+\"\\t\"+ text_b.name+'\\n')\n",
    "\n",
    "            file.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To transform data into token,label format for each train,val and test for each fold\n",
    "#Change the path to test/train/val accordingly in the input and output path mentioned below\n",
    "for i in range(10):\n",
    "    inputFile = \"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/folds/\"+str(i)+\"/train\"+str(i)+\".txt\"\n",
    "    outputFile = \"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/training/folds/\"+str(i)+\"/train.txt\"\n",
    "\n",
    "    perform_function(inputFile,outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all tests from all folds\n",
    "\n",
    "all_test_files = \"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/training/folds/\"\n",
    "outputPath = \"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/test/\"\n",
    "\n",
    "test_file_name=\"test.txt\"\n",
    "\n",
    "folders=[]\n",
    "\n",
    "for r, d, f in os.walk(all_test_files):\n",
    "    \n",
    "    for folder in d:\n",
    "        folders.append(os.path.join(r, folder))\n",
    "for paths in folders:\n",
    "    file= open(paths+\"/\"+test_file_name)\n",
    "    with open(outputPath+\"test.txt\", \"a\") as f1:\n",
    "        for line in file:\n",
    "            #print(line)\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove excess test data from each folds\n",
    "test_file_name=\"test.txt\"\n",
    "folders=[]\n",
    "for r, d, f in os.walk(all_test_files):\n",
    "    \n",
    "    for folder in d:\n",
    "        folders.append(os.path.join(r, folder))\n",
    "for paths in folders:\n",
    "    os.remove(paths+\"/test.txt\")\n",
    "    #file= open(paths+\"/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8079\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Utlity function to calculate number of lines\n",
    "fname = \"/Users/bipashabanerjee/Documents/CS/sem3/publish/styleData/train.txt\"\n",
    "# fname=\"/Users/bipashabanerjee/Documents/CS/sem3/project/styleData/journals/apa/train.txt\"\n",
    "num_lines=0\n",
    "with open(fname, 'r') as f:\n",
    "    for line in f:\n",
    "        num_lines += 1\n",
    "        \n",
    "print(num_lines)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
